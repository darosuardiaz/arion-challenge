## Overview

1. **Memory-Efficient Data Pipeline** - Real-time JSON data processing with constant memory usage
2. **Custom Context Manager** - Robust resource management with automatic cleanup
3. **Advanced Meta-Programming** - Plugin system with automatic registration and API contracts
4. **Custom Iterator with Lazy Evaluation** - Composable data transformations with minimal memory footprint
5. **Distributed Task Scheduler** - Multi-process task execution with priority queues and failure handling

### Core Components

- **`etl_pipeline/`** - Real-time data processing pipeline with windowed aggregations
- **`task_scheduler/`** - Distributed task scheduling system with priority queues
- **`resource_manager.py`** - Context managers for database and queue management
- **`lazy_iterator.py`** - Functional-style data processing with lazy evaluation
- **`plugins/`** - Metaclass-based plugin system with automatic registration
- **`etl_pipeline/webhook.py`** - HTTP API for data ingestion

### Key Features

‚úÖ **Constant Memory Usage** - Handles unlimited data volume with fixed memory footprint  
‚úÖ **Fault Tolerance** - Graceful handling of worker failures and resource cleanup  
‚úÖ **Priority Scheduling** - Task execution with configurable priority levels  
‚úÖ **Real-time Processing** - Sub-second latency for data ingestion and processing  
‚úÖ **Extensible Architecture** - Plugin system for custom data transformations  
‚úÖ **Comprehensive Testing** - Unit, integration, performance, and API tests  
‚úÖ **Production Ready** - Logging, metrics, health checks, and monitoring  

## Quick Start

### Prerequisites

- Python 3.8+

### Setup Instructions

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd arion-challenge
   ```

2. **Create a virtual environment**
   ```bash
   # Using venv (recommended)
   python -m venv venv
   
   # Activate the virtual environment
   # On macOS/Linux:
   source venv/bin/activate
   
   # On Windows:
   # venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Verify installation**
   ```bash
   python -c "import aiohttp, pytest; print('Dependencies installed successfully!')"
   ```

5. **Deactivate virtual environment (when done)**
   ```bash
   deactivate
   ```

> **Note**: Always activate the virtual environment before running the demo or tests. You'll know it's active when you see `(venv)` at the beginning of your command prompt.

### Running the Demo

The project includes a comprehensive demo script that showcases all features:

```bash
# Run the full demo (all features)
python demo.py

# Run specific feature demos
python demo.py --mode etl         # ETL pipeline only
python demo.py --mode scheduler   # Task scheduler only
python demo.py --mode lazy        # Lazy iterator only
python demo.py --mode plugins     # Plugin system only
python demo.py --mode resources   # Resource manager only
python demo.py --mode webhook     # Webhook API only
python demo.py --mode performance # Performance monitoring only
```

### Running Tests

The project includes comprehensive test suites:

```bash
# Run all tests
python run_tests.py --verbose

# Run specific test categories
python run_tests.py --unit --verbose        # Unit tests only
python run_tests.py --integration --verbose # Integration tests
python run_tests.py --performance --verbose # Performance tests
python run_tests.py --webhook --verbose     # API/Webhook tests

# Run tests with pattern matching
python run_tests.py --pattern "lazy" --verbose

# Generate test coverage report
python run_tests.py --report
```

### Basic Usage

```python
import asyncio
from etl_pipeline.pipeline import Pipeline

async def main():
    # Create pipeline with resource management
    pipeline = Pipeline(
        queue_maxsize=1000,
        window_secs=10,
        db_path="aggregates.db"
    )
    
    # Use as context manager for automatic cleanup
    with pipeline:
        # Start processing workers
        await pipeline.start_workers()
        
        # Ingest data
        event = {
            "data": {
                "category": "sales",
                "value": 100.0,
                "quantity": 5,
                "ts": "2023-01-01T00:00:00Z"
            }
        }
        await pipeline.ingest_q.put(event)
        
        # Process for a while
        await asyncio.sleep(5)
        
        # Get metrics
        metrics = pipeline.get_resource_metrics()
        print(f"Processed events: {metrics}")
        
        # Clean shutdown
        await pipeline.stop_workers()

if __name__ == "__main__":
    asyncio.run(main())
```

### Using the Webhook API

Start the HTTP server:

```bash
cd etl_pipeline
python webhook.py
```

Send data via HTTP:

```bash
curl -X POST http://localhost:8080/webhook \
  -H "Content-Type: application/json" \
  -d '{
    "data": {
      "category": "sales",
      "value": 150.0,
      "quantity": 3,
      "ts": "2023-01-01T12:00:00Z"
    }
  }'
```

Check health and metrics:

```bash
curl http://localhost:8080/healthz
curl http://localhost:8080/metrics
```

## Testing

The project includes comprehensive test suites covering unit tests, integration tests, performance tests, and API tests. See the [Running Tests](#running-tests) section above for basic usage, or check [TESTING.md](TESTING.md) for comprehensive testing documentation.

## Core Components Guide

### 1. ETL Pipeline

Real-time data processing with windowed aggregations:

```python
from etl_pipeline.pipeline import Pipeline

# Configure pipeline
pipeline = Pipeline(
    queue_maxsize=1000,    # Max events in memory
    window_secs=10,        # Aggregation window
    db_path="data.db"      # SQLite database
)

# Process events
with pipeline:
    await pipeline.start_workers()
    # Events are automatically aggregated by category
    # Results stored in SQLite with calculated metrics
```

**Features:**
- Windowed aggregations (sum, count, average)
- Backpressure handling
- Automatic database schema creation
- Resource cleanup on shutdown

### 2. Task Scheduler

Distributed task execution with priority queues:

```python
from task_scheduler.task_scheduler import TaskScheduler
from task_scheduler.task_types import Task, TaskPriority

# Create scheduler
scheduler = TaskScheduler(num_workers=4)

# Define task
def process_data(data):
    return {"result": len(data) * 2}

# Schedule with priority
task = Task(
    id="task_1",
    func=process_data,
    args=(["a", "b", "c"],),
    priority=TaskPriority.HIGH
)

# Execute
with scheduler:
    result = await scheduler.schedule_task(task)
    print(result.output)  # {"result": 6}
```

**Features:**
- Multi-process worker pool
- Priority-based execution (CRITICAL, HIGH, NORMAL, LOW)
- Task dependencies and timeouts
- Worker failure recovery
- Dynamic scaling

### 3. Lazy Iterator

Memory-efficient data transformations:

```python
from lazy_iterator import LazyIterator

# Create lazy pipeline
data = LazyIterator(range(1000000))

# Chain operations (no memory allocation yet)
result = (data
    .filter(lambda x: x % 2 == 0)
    .map(lambda x: x * 2)  
    .take(100)
    .chunk(10))

# Execute only when iterated
for chunk in result:
    print(f"Chunk: {chunk}")
```

**Operations:**
- `map(fn)` - Transform elements
- `filter(pred)` - Filter by predicate
- `take(n)` - Limit results
- `paginate(size, page)` - Pagination
- `chunk(size)` - Group into batches
- `reduce(fn, init)` - Aggregate

### 4. Plugin System

Metaclass-based automatic registration:

```python
from plugins import PluginBase

# Plugins auto-register on class definition
class DataValidator(PluginBase):
    """Validates incoming data format"""
    
    def validate(self, data):
        required = ['category', 'value', 'quantity']
        return all(field in data for field in required)
    
    def transform(self, data):
        # Normalize data format
        return {
            'category': data['category'].lower(),
            'value': float(data['value']),
            'quantity': int(data['quantity'])
        }

# Plugin automatically available
from plugins import get_all_plugins
plugins = get_all_plugins()
print(f"Registered: {[p.__name__ for p in plugins]}")
```

**Features:**
- Automatic class registration via metaclass
- API contract enforcement
- Runtime validation
- Inheritance support
- Descriptive error messages

### 5. Resource Manager

Context managers for robust resource handling:

```python
from resource_manager import ResourceManager

# Manages databases, queues, connections
manager = ResourceManager.for_pipeline(
    db_path="data.db",
    queue_maxsize=1000,
    setup_schema=True
)

with manager:
    # Database connection available
    db = manager.db
    cursor = db.execute("SELECT COUNT(*) FROM aggregates")
    
    # Queues ready for use
    queue = manager.queues["ingest_q"]
    await queue.put({"data": "example"})
    
    # Automatic cleanup on exit
```

**Features:**
- Multiple resource management
- Exception-safe cleanup
- Nested context support
- Performance metrics
- Detailed logging

## üåê API Reference

### Webhook Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/webhook` | POST | Ingest JSON data for processing |
| `/healthz` | GET | Health check with queue status |
| `/metrics` | GET | System metrics and statistics |
| `/aggregates` | GET | Query aggregated results |

### Data Format

```json
{
  "data": {
    "category": "sales",
    "value": 100.0,
    "quantity": 5,
    "ts": "2023-01-01T00:00:00Z"
  }
}
```

### Response Codes

- `202` - Accepted for processing
- `400` - Invalid JSON format
- `422` - Invalid data structure
- `429` - Queue full, retry later
- `500` - Internal server error

## üìä Performance

### Benchmarks

- **Throughput**: 10,000+ events/second
- **Latency**: <1ms median processing time  
- **Memory**: Constant usage regardless of input volume
- **Concurrency**: Scales linearly with worker count

### Performance Tests

```bash
# Run performance test suite
python run_tests.py --performance

# Custom load test
python -m tests.test_performance --events=100000 --workers=8
```

## üîß Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `QUEUE_MAXSIZE` | 1000 | Maximum events in memory |
| `WINDOW_SECS` | 10 | Aggregation window size |
| `DB_PATH` | aggregates.db | SQLite database path |
| `PORT` | 8080 | Webhook server port |
| `LOG_LEVEL` | INFO | Logging verbosity |

### Pipeline Configuration

```python
pipeline = Pipeline(
    queue_maxsize=2000,        # Memory limit
    window_secs=30,            # Longer aggregation
    db_path="/data/prod.db",   # Production database
    logger=custom_logger       # Custom logging
)
```

